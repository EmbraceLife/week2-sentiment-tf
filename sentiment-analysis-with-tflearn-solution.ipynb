{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with TFLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- building a network for sentiment analysis on the movie review data. \n",
    "- using [TFLearn](http://tflearn.org/), a high-level library built on top of TensorFlow. \n",
    "- TFLearn makes it simpler to build networks just by defining the layers. It takes care of most of the details for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 1-6 Convert a dataframe of reviews to a matrix of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### step 1\n",
    "- store multiple reviews in DataFrame dim(n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews.head(1))\n",
    "labels.head(1)\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Step 2\n",
    "- Counting word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----\n",
    "> **bag of words**\n",
    "- **count how often** each word appears in the data\n",
    "- use this count to create a vocabulary which we'll use to **encode the review data**. \n",
    "- This resulting count is known as a [**bag of words**](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "- use it to select our vocabulary and build the word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Exercise:** Create the bag of words from the reviews data. \n",
    "- The reviews are stores in the `reviews` [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). \n",
    "- If you want the reviews as **a Numpy array**, use `reviews.values`. \n",
    "- You can **iterate through the rows in the DataFrame** with `for idx, row in reviews.iterrows():` ([documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  74074\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "total_counts = Counter()\n",
    "for _, row in reviews.iterrows():\n",
    "    total_counts.update(row[0].split(' '))\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Step 3: \n",
    "- sort the counter and keep the most common 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----\n",
    "> **keep only the most frequent words**\n",
    "- keep the first 10000 most frequent words.\n",
    "- most of the words in the vocabulary are rarely used \n",
    "- so they will have little effect on our predictions\n",
    "- sort `vocab` by the count value and keep the 10000 most frequent words.\n",
    "- by now, `vocab` is **total unique word-count of entire reviews set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336713"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts.get('the') # like dict.get, checked\n",
    "total_counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'the', '.', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you', 'on', 't', 'not', 'he', 'are', 'his', 'have', 'be', 'one', 'all', 'at', 'they', 'by', 'an', 'who', 'so', 'from', 'like', 'there', 'her', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more', 'she', 'when', 'very', 'up', 'time', 'no']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000] # checked\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----\n",
    "> **How to know whether keep 10000 most common words are enough**\n",
    "- What's the last word in our vocabulary?\n",
    "- We can use this to judge if 10000 is too few. \n",
    "- If the last word is pretty common, we probably need to keep more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intrusive :  30\n"
     ]
    }
   ],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----\n",
    "> **maybe range from 20 to 50 is fine**\n",
    "- The last word in our vocabulary shows up in 30 reviews out of 25000. \n",
    "- I think it's fair to say this is a tiny proportion of reviews. \n",
    "- We are probably fine with this number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Step 4\n",
    "- from word Counter to word-index dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----\n",
    "> **Build a word-index for all unique words of entire reviews set**\n",
    "- Now for each review in the data, we'll make a word vector. \n",
    "- First we need to make a mapping of word to index, pretty easy to do with a dictionary comprehension.\n",
    "\n",
    "> **create a dictionary `word2idx`**\n",
    "- maps each word in the total vocabulary set to an index of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word2idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Step 5\n",
    "- Convert a review to a vector index == unique word, value == count of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "- write a function that converts a string sentence to a word vector. The function will take a string of words as input and return a vector with the words counted up. Here's the general algorithm to do this:\n",
    "\n",
    "* Initialize the word vector with [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html), it should be the length of the vocabulary.\n",
    "* Split the input string of text into a list of words with `.split(' ')`.\n",
    "* For each word in that list, increment the element in the index associated with that word, which you get from `word2idx`.\n",
    "\n",
    "**Note:** Since all words aren't in the `vocab` dictionary, you'll get a key error if you run into one of those words. You can use the `.get` method of the `word2idx` dictionary to specify a default returned value when you make a key error. For example, `word2idx.get(word, None)` returns `None` if `word` doesn't exist in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    \n",
    "    # build a vector of zero with length of total vocab\n",
    "    word_vector = np.zeros(len(vocab), dtype=np.int_)\n",
    "    \n",
    "    # loop every word of a review\n",
    "    for word in text.split(' '):\n",
    "        # if the word is part of total vocab, it has a index\n",
    "        # assign its index to idx, otherwise, assign None to idx\n",
    "        idx = word2idx.get(word, None)\n",
    "        if idx is None:\n",
    "            continue # this element remains zero\n",
    "            \n",
    "        # if the word has an index, then add 1 count to its place in the vector    \n",
    "        else:\n",
    "            word_vector[idx] += 1 # this element keep count\n",
    "            \n",
    "    # by now, we got a vector: vector index refers to unique word, vector value refer to\n",
    "    # occurance of the word\n",
    "    return np.array(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you do this right, the following code should return\n",
    "\n",
    "```\n",
    "sentence_to_vector('The tea is for a party to celebrate '\n",
    "                   'the movie so she has no time for a cake')[:65]\n",
    "                   \n",
    "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
    "```       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_vector('The tea is for a party to celebrate '\n",
    "                   'the movie so she has no time for a cake')[:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Step 6\n",
    "- Now, run through our entire review data set and convert each review to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build a matrix to store all reviews (on rows), and all words (on columns)\n",
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_) # set dtype\n",
    "\n",
    "# take each review and convert it to a vector for each row\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])\n",
    "    \n",
    "# by now, word_vectors is a numpy matrix with all reviews converted to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,   9,  27,   1,   4,   4,   6,   4,   0,   2,   2,   5,   0,\n",
       "          4,   1,   0,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  5,   4,   8,   1,   7,   3,   1,   2,   0,   4,   0,   0,   0,\n",
       "          1,   2,   0,   0,   1,   3,   0,   0,   0,   1],\n",
       "       [ 78,  24,  12,   4,  17,   5,  20,   2,   8,   8,   2,   1,   1,\n",
       "          2,   8,   0,   5,   5,   4,   0,   2,   1,   4],\n",
       "       [167,  53,  23,   0,  22,  23,  13,  14,   8,  10,   8,  12,   9,\n",
       "          4,  11,   2,  11,   5,  11,   0,   5,   3,   0],\n",
       "       [ 19,  10,  11,   4,   6,   2,   2,   5,   0,   1,   2,   3,   1,\n",
       "          0,   0,   0,   3,   1,   0,   1,   0,   0,   0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first 5 reviews (5 rows), and first 23 words-count(first 23 columns)\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 Train, Validation, Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split our data into train, validation, and test sets. \n",
    "- train on the train data\n",
    "- use the validation data to set the hyperparameters\n",
    "- at the very end measure the network performance on the test data. \n",
    "- `to_categorical` from TFLearn to reshape the target data\n",
    "- have two output units and can classify with a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# positive as 1, negative as 0\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "# get index for each review\n",
    "shuffle = np.arange(records)\n",
    "\n",
    "# shuffle index of all reviews\n",
    "np.random.shuffle(shuffle)\n",
    "\n",
    "# get index for training part and test part\n",
    "test_fraction = 0.9\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "\n",
    "# get matrix for training set from total-review-matrix, and convert training set label to num\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "\n",
    "# get matrix for test set from total-review-matrix, and convert test set label to num\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY # it is like one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- [TFLearn](http://tflearn.org/) lets you build the network by [defining the layers](http://tflearn.org/layers/core/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the input layer, you just need to tell it how many units you have\n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 100])\n",
    "```\n",
    "- place of None: None as default, or set your mini-batch size\n",
    "- place of 100: set num of input neurons you want (must match num of attributes/features of your data)\n",
    "- set 10000, as we have 10000 unique words in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To add new hidden layers, you use \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "- adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer\n",
    "- The first argument `net` is the network you created in the `tflearn.input_data` call\n",
    "- set the number of units in the layer with `n_hidden`\n",
    "- set the activation function with the `activation` keyword. \n",
    "- keep adding layers to your network by repeated calling `net = tflearn.fully_connected(net, n_units)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add is used as the output layer\n",
    "- set the number of units to match the target data\n",
    "- predicting two classes, positive or negative sentiment\n",
    "- we're trying to predict if some input data belongs to one of two classes, so we should use softmax.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "----\n",
    "> put all this together to create the model\n",
    "- using `tflearn.DNN(net)`. \n",
    "- So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 10])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Inputs\n",
    "    net = tflearn.input_data([None, 10000])\n",
    "\n",
    "    # Hidden layer(s)\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')\n",
    "\n",
    "    # Output layer\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    \n",
    "    # how to train\n",
    "    net = tflearn.regression(net, optimizer='sgd', \n",
    "                             learning_rate=0.1, \n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing the model\n",
    "\n",
    "Next we need to call the `build_model()` function to actually build the model. In my solution I haven't included any arguments to the function, but you can add arguments so you can change parameters in the model if you want.\n",
    "\n",
    "> **Note:** You might get a bunch of warnings here. TFLearn uses a lot of deprecated code in TensorFlow. Hopefully it gets updated to the new TensorFlow version soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Natsume/miniconda2/envs/dlnd-tf/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /Users/Natsume/miniconda2/envs/dlnd-tf/lib/python3.5/site-packages/tflearn/summaries.py:46 in get_summary.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From /Users/Natsume/miniconda2/envs/dlnd-tf/lib/python3.5/site-packages/tflearn/helpers/trainer.py:766 in create_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\n",
      "WARNING:tensorflow:From /Users/Natsume/miniconda2/envs/dlnd-tf/lib/python3.5/site-packages/tflearn/helpers/trainer.py:130 in __init__.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. Here we use the `model.fit` method. You pass in the training features `trainX` and the training targets `trainY`. Below I set `validation_set=0.1` which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the `batch_size` and `n_epoch` keywords, respectively. Below is the code to fit our the network to our word vectors.\n",
    "\n",
    "You can rerun `model.fit` to train the network further if you think you can increase the validation accuracy. Remember, all hyperparameter adjustments must be done using the validation set. **Only use the test set after you're completely done training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7950  | total loss: \u001b[1m\u001b[32m0.32508\u001b[0m\u001b[0m\n",
      "| SGD | epoch: 050 | loss: 0.32508 - acc: 0.8542 | val_loss: 0.47160 - val_acc: 0.8178 -- iter: 20250/20250\n",
      "Training Step: 7950  | total loss: \u001b[1m\u001b[32m0.32508\u001b[0m\u001b[0m\n",
      "| SGD | epoch: 050 | loss: 0.32508 - acc: 0.8542 | val_loss: 0.47160 - val_acc: 0.8178 -- iter: 20250/20250\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training the model - checked\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "After you're satisified with your hyperparameters, you can run the network on the test set to measure it's performance. Remember, *only do this after finalizing the hyperparameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8276\n"
     ]
    }
   ],
   "source": [
    "# read carefully about the use between ndarray and list here\n",
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.519019603729248, 0.4809803068637848],\n",
       " [0.707361102104187, 0.2926388680934906]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how to access element of numpy arrays 2-d\n",
    "model.predict(testX)[0][1]\n",
    "model.predict(testX)[0:2][1]\n",
    "model.predict(testX)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(positive) = 0.927 : Positive\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Moonlight is by far the best movie of 2016.\"\n",
    "positive_prob = model.predict([text_to_vector(sentence.lower())])[0][1]\n",
    "print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "      'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/s7FKYC5Zcm8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/s7FKYC5Zcm8?ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
